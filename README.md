**Important:** Due to file sizes, lora_adapters can be reached through here: https://drive.google.com/drive/folders/1M_zupYKwMso5r5ibYMzNiTxg--89sf-0?usp=drive_link

# A Hybrid Post-Hoc Feedback Framework for Latin Dactylic Hexameter

This research investigates the challenge of generating metrically correct Latin dactylic hexameter on contemporary themes using modern transformer models. The core of this research is a hybrid system that combines the creative potential of fine-tuned Large Language Models (LLMs) with the formal rigor of a symbolic, rule-based validator.

A series of experiments were conducted across several models from the Llama and Gemma families. These models were fine-tuned on four custom datasets, each designed for a specific purpose: V1/V2 established a baseline with core classical authors and tested the use of special tokens; V3 expanded the corpus with more authors to improve generalization; and V4 introduced synthetically generated poems on modern topics to adapt an instruction-tuned model for contemporary themes. The generation process uses an iterative feedback loop: the LLM generates a line of poetry, which is then validated for metrical correctness by the Classical Language Toolkit (CLTK). If a line fails validation, it is discarded and regenerated.

The findings indicate that larger, instruction-tuned models like Gemma 3 12B-IT produce the most semantically coherent and stylistically convincing verse. While many models were tested, the final generation code in 3_Poetry_Generator.ipynb is specifically configured for the two most successful models: gemma-3-12b-pt (for classical generation) and gemma-3-12b-it (for contemporary generation). However, achieving consistent metrical accuracy proved challenging, primarily due to limitations in the CLTK's validation tools. This research involves a framework for computational creativity in historical languages and provides a detailed analysis of the current capabilities and limitations of LLMs in handling complex poetic forms.

## Datasets (/data)

*   **scraped_poetry** : This folder contains the raw, cleaned texts scraped from The Latin Library. It includes all the authors.
*   **dataset_v1** : This was the initial baseline corpus, consisting of works by Virgil (Aeneid) and Ovid (Metamorphoses). It contains approximately 22,000 lines of classical dactylic hexameter.
*   **dataset_v2** : This dataset uses the same classical texts as V1 but incorporates special "trigger" tokens to signal the beginning and end of a poetic passage. This formatting was essential for controlling the output of the pretrained models. It has two tokenized versions for both Llama 3.2 and Gemma 3 models.
*   **dataset_V3** : To improve model generalization and prevent overfitting on larger models, this version expands the corpus to approximately 56,000 lines. It includes the works of Statius, Lucan, Silius Italicus, and Juvenal in addition to Virgil and Ovid. It still uses "trigger" tokens and has two different tokenized versions.
*   **dataset_v4** : This is the most specialized dataset, designed exclusively for the instruction-tuned gemma-modern model. It augments the V3 corpus with over 200 synthetically generated Latin poems (by GPT-4o) on contemporary themes. The entire dataset is formatted into a conversational chat structure that the instruction-tuned model expects, with distinct prompts for generating classical versus modern poetry.

## Generated Passages (/generations)

*   **1_Gemma-modern_passages_on_contemporary_themes.txt** : Contains 50 four-line passages generated by the gemma-modern model (Gemma 3 12B IT fine-tuned on Dataset V4). These passages were generated in response to prompts about modern topics, such as 'science & technology' and 'everyday modern life'.
*   **2_Gemma-large_passages.txt** : Contains passages from the gemma-large model (Gemma 3 12B PT fine-tuned on Dataset V3). This model represents the best-performing pretrained "generalist."
*   **3_Llama-large_passages.txt** : Includes passages from the llama-large model (Llama 3.2 3B Latin fine-tuned on Dataset V3). This model serves as the Llama-family "generalist," trained on the expanded corpus.
*   **4_Llama-small_passages.txt** : Contains passages from the llama-small model (Llama 3.2 3B Latin fine-tuned on Dataset V2). This model acts as the baseline "specialist," trained only on the core classical authors.

## Model Performance Summary

*    **Llama-small** and **Llama-large** models, which served as the baseline and generalist for their family, consistently struggled to produce more than syntactically flawed fragments that mimicked the style of Latin epic without grammatical coherence. The 


*    **Gemma-large** model demonstrated a significant improvement, leveraging a more advanced architecture to generate more semantically coherent and stylistically sophisticated passages, though it often failed to complete a full thought. Finally, the 

*    **Gemma-modern** model proved to be the most successful, using its instruction-tuned design to produce complete, thematically controlled, and grammatically better sounding passages on contemporary topics.


**This progression underscores that while advanced LLMs can be successfully adapted for complex creative tasks, the formal correctness of their output was ultimately compromised by the limitations of the symbolic CLTK validator, whose hard-coded assumptions and tagging inaccuracies proved insufficient for the nuances of Latin prosody.**
