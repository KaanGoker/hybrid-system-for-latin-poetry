{"cells":[{"cell_type":"markdown","source":["This notebook outlines the data preparation pipeline for the Latin poetry corpus. It covers the three main stages:\n","\n","\n","\n","*   Automated web scraping of texts from The Latin Library\n","*   Preprocessing to clean and normalize the raw data\n","*   Final dataset formatting, where the text is chunked and tokenized to create training, validation, and test sets for fine-tuning\n","\n","Additionally,\n","\n","*   Document covers the generation of synthetic poetry using GPT-4, which was a specific step required for the creation of Dataset V4"],"metadata":{"id":"rpCTQKlAc4rU"},"id":"rpCTQKlAc4rU"},{"cell_type":"markdown","source":["# Imports\n","\n"],"metadata":{"id":"Colt63hVfdbm"},"id":"Colt63hVfdbm"},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","from pathlib import Path\n","import re\n","import unicodedata\n","import os\n","from transformers import AutoTokenizer\n","from datasets import Dataset, DatasetDict"],"metadata":{"id":"s7cp4jiffdNS"},"id":"s7cp4jiffdNS","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. Scraping & Preprocessing"],"metadata":{"id":"QyNqnyEZmpSq"},"id":"QyNqnyEZmpSq"},{"cell_type":"markdown","source":["To build the dataset, the Latin poetry texts were programmatically collected from The Latin Library. The process involved creating a web scraping script in Python to automate the download and cleaning of the texts.\n","\n","The script shown below was developed to handle this task. It is demonstrated here using Ovid's Metamorphoses as the primary example."],"metadata":{"id":"eIe8l4ijmkOt"},"id":"eIe8l4ijmkOt"},{"cell_type":"markdown","source":["The script executes a series of steps for each book of a given work:\n","\n","*   Requesting Content: It sends an HTTP request to the specific URL for each book, mimicking a web browser to ensure a successful response.\n","*   Parsing HTML: Upon receiving the webpage's content, it uses the BeautifulSoup library to parse the raw HTML and extract all the text.\n","*   Text Cleaning and Normalization: This is the most critical step. The raw text is processed to make it suitable for analysis:\n","*   Irrelevant Content Removal: Headers, footers, titles, and any lines written in all caps (which are typically headings) are filtered out.\n","*   Whitespace and Unicode Normalization: All forms of whitespace are standardized to single spaces, and Unicode characters are normalized to a consistent format.\n","*   Line Number Removal: The script uses regular expressions to identify and remove the line numbers that are often embedded at the end or even in the middle of lines in the source text.\n","*   Paragraph Detection: Paragraph breaks in the original text, indicated by indentation, are detected and preserved by adding a blank line.\n","*   Saving the Data: Once cleaned, the lines of poetry for each book are saved into a separate .txt file, organized into a directory named after the author and the work."],"metadata":{"id":"4nMuAK7hnwYL"},"id":"4nMuAK7hnwYL"},{"cell_type":"markdown","source":["The script's parameters (such as the author name and URL structure) were adjusted for each of the following works:\n","\n","\n","*   Virgil, Aeneid\n","*   Statius, Thebaid\n","*   Lucan, Pharsalia\n","*   Silius Italicus, Punica\n","*   Valerius Flaccus, Argonautica\n","*   Juvenal, Satires"],"metadata":{"id":"nXl-zQZ-pF4Q"},"id":"nXl-zQZ-pF4Q"},{"cell_type":"markdown","source":["## Ovid - Metamorphoses"],"metadata":{"id":"kYc1lvDIfyHm"},"id":"kYc1lvDIfyHm"},{"cell_type":"code","source":["# Path\n","base_path = Path(\"C:/Users/nadir/Desktop/ClassicalLatinPoetryGeneration/datasets/HexameterPoetry/Ovid-Metamorphoses\")\n","base_path.mkdir(parents=True, exist_ok=True)\n","\n","# Loop through all books\n","for book_num in range(1, 16):\n","    url = f\"http://www.thelatinlibrary.com/ovid/ovid.met{book_num}.shtml\"\n","    print(f\"ðŸ“¥ Processing Book {book_num} from {url}\")\n","\n","    headers = {\n","        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36\"\n","    }\n","\n","    response = requests.get(url, headers=headers)\n","    soup = BeautifulSoup(response.content, \"html.parser\")\n","    raw_lines = soup.get_text().splitlines()\n","\n","    poem_lines = []\n","\n","    for line in raw_lines:\n","        original_line = line  # Keep indentation for paragraph detection\n","        line = line.rstrip()\n","\n","        is_paragraph_start = bool(re.match(r\"^\\s{3,}\", original_line))\n","        line = line.strip()\n","\n","        # Skip irrelevant lines\n","        if (\n","            not line\n","            or line.isupper()\n","            or line.startswith(\"â€”\")\n","            or line.lower().startswith(\"ovid\")\n","        ):\n","            continue\n","\n","        # Unicode normalization\n","        line = unicodedata.normalize(\"NFKC\", line)\n","\n","        # Remove trailing numbers\n","        line = re.sub(r\"\\s{2,}\\d+[a-zA-Z]?$\", \"\", line)\n","        line = re.sub(r\"\\s{2,}\\d{1,3}$\", \"\", line)\n","\n","        # Normalize whitespace\n","        line = re.sub(r\"[Â ]+\", \" \", line)\n","        line = re.sub(r\"\\s{2,}\", \" \", line)\n","        line = line.replace(\"â€™\", \"'\")\n","\n","        # Filter by reasonable poetic line length\n","        if not (25 <= len(line) <= 120):\n","            continue\n","\n","        # Mid-line numbering split\n","        mid_line_split = re.split(r\"(\\s[.,;:!?]?\\s*\\d{1,3}\\s+)\", line)\n","        if len(mid_line_split) > 1:\n","            parts = [part.strip() for part in mid_line_split if part.strip() and not part.strip().isdigit()]\n","            if is_paragraph_start and len(poem_lines) > 0:\n","                poem_lines.append(\"\")\n","            poem_lines.extend(parts)\n","        else:\n","            if is_paragraph_start and len(poem_lines) > 0:\n","                poem_lines.append(\"\")\n","            poem_lines.append(line)\n","\n","    print(f\"   â†’ {len(poem_lines)} cleaned lines from Book {book_num}\")\n","\n","    # Saving\n","    output_file = base_path / f\"ovid_metamorphoses-book-{book_num}.txt\"\n","    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n","        f.write(\"\\n\".join(poem_lines))\n","\n","    print(f\" Saved to {output_file}\")"],"metadata":{"id":"VtTfaeQMfe3Y"},"id":"VtTfaeQMfe3Y","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Merging All Books into One File"],"metadata":{"id":"WkQf5WqUopub"},"id":"WkQf5WqUopub"},{"cell_type":"code","source":["# Path\n","base_dir = Path(\"C:/Users/nadir/Desktop/ClassicalLatinPoetryGeneration/datasets/HexameterPoetry\")\n","\n","# Output path\n","output_dir = base_dir / \"Dataset V3\"\n","output_dir.mkdir(parents=True, exist_ok=True)\n","output_file = output_dir / \"latin_poetry_all.txt\"\n","\n","# 3. List the subfolders\n","folders_to_process = [\n","    \"Ovid-Metamorphoses\",\n","    \"Virgil-Aeneid\",\n","    \"Statius-Thebaid\",\n","    \"Lucan-Pharsalia\",\n","    \"Silius-Punica\",\n","    \"Juvenal-Satires\"\n","]\n","\n","all_books_content = []\n","\n","# Go through each folder\n","for folder_name in folders_to_process:\n","    folder_path = base_dir / folder_name\n","        continue\n","\n","    # This key function splits 'author-work-book-12' at '-' and takes the last part ('12').\n","    book_files = sorted(folder_path.glob(\"*.txt\"), key=lambda p: int(p.stem.split(\"-\")[-1]))\n","\n","    # Read each sorted book file.\n","    for file_path in book_files:\n","        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","            # Read the entire content of the book.\n","            book_text = f.read().strip()\n","            # Add the book's text to our master list.\n","            all_books_content.append(book_text)\n","\n","# Combine all the individual book texts into one large string.\n","# Each book will be separated by two newlines, which is a good separator for training.\n","combined_text = \"\\n\\n\".join(all_books_content)\n","\n","# Saving\n","with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n","    out_f.write(combined_text)"],"metadata":{"id":"wEze5h3Gf8_Q","executionInfo":{"status":"ok","timestamp":1755242998659,"user_tz":-120,"elapsed":16,"user":{"displayName":"Kaan Goker","userId":"00010978088592225673"}}},"id":"wEze5h3Gf8_Q","execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# 3. Chunking Paragraphs with Tokenizer"],"metadata":{"id":"oJcG6HUaqHgd"},"id":"oJcG6HUaqHgd"},{"cell_type":"markdown","source":["The script:\n","\n","*   Uses max token length of 512.\n","*   It defines \"< unused0 >\" and \"< unused1 >\" as special markers. These will be used to tell the model exactly where a piece of poetry begins and ends, which helps it learn the structure of the data.\n","\n","It reads text and splits it into paragraphs. Then, for each paragraph, it checks its length in tokens:\n","\n","*   If a paragraph is short enough to fit within the EFFECTIVE_MAX_LENGTH, it's kept as is.\n","\n","*   If a paragraph is too long, the script intelligently breaks it down into smaller \"chunks.\" It does this carefully, line by line, to ensure that none of the new chunks exceed the token limit.\n","\n","After created chunks shuffled randomly, they divided into: training set (80%), validation set (10%), test set (10%).\n","\n","This specific script is designed for Gemma 3 models (they all use same tokenizer), a very similar script is used for Llama models.\n"],"metadata":{"id":"O_XLnS_RGlCl"},"id":"O_XLnS_RGlCl"},{"cell_type":"code","source":["# Configuration for Gemma Dataset V3\n","random.seed(42)\n","\n","# Max length for the text is 512 - 4 (BOS, EOS, unused0, unused1) - 2 (safety margin)\n","EFFECTIVE_MAX_LENGTH = 506\n","\n","# Using the Gemma 3 tokenizer\n","TOKENIZER_PATH = r\"C:\\Users\\nadir\\Desktop\\ClassicalLatinPoetryGeneration\\models\\gemma-3-27b-it-qat-q4_0-unquantized\"\n","\n","base_dir = Path(r\"C:\\Users\\nadir\\Desktop\\ClassicalLatinPoetryGeneration\\datasets\\HexameterPoetry\\Dataset V3\")\n","input_file = base_dir / \"latin_poetry_all.txt\"\n","\n","output_dir = Path(r\"C:\\Users\\nadir\\Desktop\\ClassicalLatinPoetryGeneration\\datasets\\HexameterPoetry\\Dataset V3 Gemma\")\n","output_dir.mkdir(exist_ok=True)\n","\n","# Special Tokens for Gemma 3 models\n","start_token = \"<unused0>\"\n","end_token = \"<unused1>\""],"metadata":{"id":"5Fgk24mIofUI"},"id":"5Fgk24mIofUI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenizer Loading\n","tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n","\n","# Script\n","with open(input_file, \"r\", encoding=\"utf-8\") as f:\n","    full_text = f.read().strip()\n","\n","original_paragraphs = [p.strip() for p in full_text.split(\"\\n\\n\") if p.strip()]\n","processed_paragraphs = []\n","\n","for para in original_paragraphs:\n","    # Check length of the text only\n","    para_tokens = tokenizer.encode(para, add_special_tokens=False)\n","\n","    if len(para_tokens) <= EFFECTIVE_MAX_LENGTH:\n","        processed_paragraphs.append(para)\n","    else:\n","        # Split long paragraphs into smaller chunks\n","        lines = para.split('\\n')\n","        current_chunk_tokens = []\n","        current_chunk_text = \"\"\n","\n","        for line in lines:\n","            line_tokens = tokenizer.encode(('\\n' if current_chunk_text else '') + line, add_special_tokens=False)\n","\n","            if len(current_chunk_tokens) + len(line_tokens) > EFFECTIVE_MAX_LENGTH:\n","                processed_paragraphs.append(current_chunk_text)\n","                current_chunk_text = line\n","                current_chunk_tokens = tokenizer.encode(line, add_special_tokens=False)\n","            else:\n","                current_chunk_text += ('\\n' if current_chunk_text else '') + line\n","                current_chunk_tokens.extend(line_tokens)\n","\n","        processed_paragraphs.append(current_chunk_text)\n","\n","# Shuffle and Split\n","random.shuffle(processed_paragraphs)\n","n = len(processed_paragraphs)\n","train_split = processed_paragraphs[:int(n * 0.8)]\n","val_split = processed_paragraphs[int(n * 0.8):int(n * 0.9)]\n","test_split = processed_paragraphs[int(n * 0.9):]\n","\n","# Formatting and Saving\n","def format_and_save(paragraphs, file_path):\n","    # Wrap each paragraph/chunk with the special tokens\n","    formatted_paragraphs = [f\"{start_token}\\n{p}\\n{end_token}\" for p in paragraphs]\n","    content_to_save = \"\\n\\n\".join(formatted_paragraphs)\n","    with open(file_path, 'w', encoding='utf-8') as f:\n","        f.write(content_to_save)\n","\n","format_and_save(train_split, output_dir / \"train.txt\")\n","format_and_save(val_split, output_dir / \"val.txt\")\n","format_and_save(test_split, output_dir / \"test.txt\")"],"metadata":{"id":"WsusQT4UofRz","executionInfo":{"status":"ok","timestamp":1755251269171,"user_tz":-120,"elapsed":3,"user":{"displayName":"Kaan Goker","userId":"00010978088592225673"}}},"id":"WsusQT4UofRz","execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# 4. Tokenizing Datasets"],"metadata":{"id":"rGYDBRRrJBUX"},"id":"rGYDBRRrJBUX"},{"cell_type":"markdown","source":["Very similar script used for Llama models."],"metadata":{"id":"8_FawYGUL6Gd"},"id":"8_FawYGUL6Gd"},{"cell_type":"code","source":["# Input directory containing text files formatted with <unused0> and <unused1>\n","input_dir = r\"C:\\Users\\nadir\\Desktop\\ClassicalLatinPoetryGeneration\\datasets\\HexameterPoetry\\Dataset V3 Gemma\"\n","# Final output directory for the tokenized Gemma Dataset V3\n","output_dir = r\"C:\\Users\\nadir\\Desktop\\ClassicalLatinPoetryGeneration\\datasets\\HexameterPoetry\\Dataset V3 Gemma\\tokenized\"\n","\n","TRAIN_FILE_PATH = os.path.join(input_dir, \"train.txt\")\n","VAL_FILE_PATH = os.path.join(input_dir, \"val.txt\")\n","TEST_FILE_PATH = os.path.join(input_dir, \"test.txt\")\n","\n","# Set Padding Side to 'right'\n","tokenizer.padding_side = \"right\"\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Functions\n","def create_paragraph_list(file_path):\n","    \"\"\"Reads a text file and splits it into a list of pre-formatted paragraphs/chunks.\"\"\"\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        content = f.read()\n","\n","    # The paragraphs are already formatted with our special tokens\n","    paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n","    return paragraphs\n","\n","def tokenize_function(examples):\n","    \"\"\"Tokenizes the pre-formatted text, adding the final EOS token for the model.\"\"\"\n","    # The text already has our start/end tokens. We just add the final EOS token.\n","    # The tokenizer will also automatically add the BOS token.\n","    formatted_texts = [f\"{text}{tokenizer.eos_token}\" for text in examples[\"text\"]]\n","\n","    tokenized_outputs = tokenizer(\n","        formatted_texts,\n","        truncation=True,\n","        max_length=512,\n","        padding=\"max_length\"\n","    )\n","\n","    tokenized_outputs[\"labels\"] = tokenized_outputs[\"input_ids\"].copy()\n","\n","    return tokenized_outputs\n","\n","\n","# Tokenizing\n","\n","train_paragraphs = create_paragraph_list(TRAIN_FILE_PATH)\n","val_paragraphs = create_paragraph_list(VAL_FILE_PATH)\n","test_paragraphs = create_paragraph_list(TEST_FILE_PATH)\n","\n","raw_datasets = DatasetDict({\n","    'train': Dataset.from_dict({'text': train_paragraphs}),\n","    'validation': Dataset.from_dict({'text': val_paragraphs}),\n","    'test': Dataset.from_dict({'text': test_paragraphs})\n","})\n","\n","tokenized_datasets = raw_datasets.map(\n","    tokenize_function,\n","    batched=True,\n","    remove_columns=[\"text\"]\n",")\n","\n","# Saving\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","tokenized_datasets.save_to_disk(output_dir)"],"metadata":{"id":"MDqp7fmKofMn"},"id":"MDqp7fmKofMn","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. \"Modern\" Poems"],"metadata":{"id":"a3OJ3B4bMYIj"},"id":"a3OJ3B4bMYIj"},{"cell_type":"markdown","source":["This section outlines the specific and additional preprocessing steps that were undertaken to create Dataset V4.\n","\n","Since Dataset V4 created for an instruct model, following templates used for each chunk:\n","\n","\n","```\n","# Structure for Classical Poem Chunk:\n","<start_of_turn>user\n","Write a poem in Latin, dactylic hexameter style. Just type the poem. Nothing more.<end_of_turn>\n","<start_of_turn>model\n","[A classical Latin poetry paragraph]<end_of_turn>\n","\n","# Structure for Modern Poem Chunk:\n","<start_of_turn>user\n","Write a poem in Latin, on a modern topic, in dactylic hexameter style. Just type the poem. Nothing more.<end_of_turn>\n","<start_of_turn>model\n","[A synthetically generated modern Latin poem]<end_of_turn>\n","```\n","\n"],"metadata":{"id":"2NQ8guKVNKpA"},"id":"2NQ8guKVNKpA"},{"cell_type":"markdown","source":["## Creating Poems with GPT-4"],"metadata":{"id":"4BpyQBLpPn3d"},"id":"4BpyQBLpPn3d"},{"cell_type":"code","source":["from openai import OpenAI\n","from pathlib import Path\n","import time\n","import os"],"metadata":{"id":"1B_uin_SPsHa"},"id":"1B_uin_SPsHa","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import userdata\n","userdata.get('GPT')"],"metadata":{"id":"oFn-IIboNJ1q","executionInfo":{"status":"ok","timestamp":1755253057056,"user_tz":-120,"elapsed":4,"user":{"displayName":"Kaan Goker","userId":"00010978088592225673"}}},"id":"oFn-IIboNJ1q","execution_count":3,"outputs":[]},{"cell_type":"code","source":["API_KEY_FILE = \"/content/drive/MyDrive/ClassicalLatinPoetryGeneration/openai_api_key.txt\"\n","\n","base_dir = Path(\"/content/drive/MyDrive/ClassicalLatinPoetryGeneration/DatasetV4-IT/DatasetV4/\")\n","output_file = base_dir / \"latin_poetry_modern_generated.txt\""],"metadata":{"id":"wOBVH_72PqyC"},"id":"wOBVH_72PqyC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Topics\n","\n","modern_topics = [\n","    # Technology & Science\n","    \"a smartphone\", \"a rocket launching to the moon\", \"the internet connecting the world\",\n","    \"a self-driving car\", \"a social media feed\", \"a video game world\", \"a quantum computer\",\n","    \"a drone flying over a landscape\", \"a virtual reality headset\", \"an artificial intelligence writing code\",\n","    \"a 3D printer creating an object\", \"a satellite orbiting Earth\", \"a cryptocurrency transaction\",\n","    \"a genetic engineering lab\", \"a particle accelerator\", \"an online encyclopedia\", \"a GPS navigation system\",\n","    \"a data center server rack\", \"a software update\", \"a Mars rover exploring a crater\",\n","    \"a neural network learning\", \"a satellite internet constellation\", \"an augmented reality map\",\n","    \"a smart city's sensor network\", \"a space telescope discovering a new galaxy\", \"a robotic surgeon\",\n","    \"a brain-computer interface\",\n","\n","    # Everyday Modern Life\n","    \"a cup of coffee in the morning\", \"a remote work video call\", \"a bustling airport terminal\",\n","    \"a quiet electric car\", \"a traffic jam on a highway\", \"a quiet library with computers\",\n","    \"a home security camera\", \"an airplane taking off\", \"a skyscraper elevator\",\n","    \"a credit card payment\", \"a barcode scanner\", \"an online shopping cart\",\n","    \"a fitness tracker\", \"a smart home assistant\", \"a self-checkout lane\",\n","    \"a food delivery app\", \"a streaming movie service\", \"a high-speed train\",\n","    \"a ride-sharing service\", \"a coffee shop filled with people on laptops\",\n","    \"a crowded subway car during rush hour\", \"an online food recipe\", \"a home-delivered package\",\n","    \"a podcast playing on a commute\", \"a digital family photo album\", \"a video call with a grandparent\",\n","    \"a self-service gas station\", \"a modern gym with treadmills\", \"a quiet Sunday morning in a city apartment\",\n","    \"an online language learning app\", \"a smart thermostat adjusting the temperature\", \"a meal-kit delivery box\",\n","    \"a dashcam recording a journey\", \"a contactless payment at a store\", \"a pop-up advertisement on a screen\",\n","    \"a fast food restaurant's drive-thru\", \"an e-book reader at night\",\n","\n","    # Romantic Love & Existentialism (Expanded)\n","    \"the beginning of a new romance\", \"a quiet, intimate moment with a loved one\", \"the heartbreak of a breakup\",\n","    \"unrequited love in the age of social media\", \"a modern wedding ceremony\", \"the search for purpose in a vast universe\",\n","    \"contemplating one's own mortality\", \"the feeling of absolute freedom and its resulting dread\", \"the absurdity of a daily office routine\",\n","    \"creating personal meaning in a meaningless world\", \"the quiet comfort of a long-term relationship\",\n","    \"the jealousy sparked by a social media post\", \"the decision to end a relationship\", \"a first date's awkwardness and hope\",\n","    \"remembering a lost love\", \"the feeling of being insignificant under the stars\", \"the weight of making a life-altering decision\",\n","    \"the conflict between free will and determinism\", \"the human desire to leave a legacy\", \"the quiet beauty of an ordinary day\",\n","    \"the fear of being forgotten after death\", \"the search for authenticity in a superficial world\",\n","    \"the feeling of being a stranger in one's own life\", \"the fleeting nature of happiness\", \"a love letter sent as a text message\",\n","    \"the comfortable silence between old lovers\", \"the pain of betrayal\", \"the challenge of forgiving someone\",\n","    \"wondering about the 'what ifs' of past choices\", \"the feeling of time passing too quickly\", \"finding beauty in imperfection\",\n","\n","    # Contemporary Experiences & Emotions\n","    \"the feeling of urban loneliness\", \"a long-distance relationship\", \"the anxiety of a job interview\",\n","    \"the quiet of a city after a rainstorm\", \"a protest for social change\", \"the memory of a past love\",\n","    \"the hope for a better future\", \"the stress of modern work life\", \"a moment of unexpected kindness\",\n","    \"the feeling of nostalgia for childhood\", \"the joy of a small success\", \"the fear of climate change\",\n","    \"a difficult moral choice\", \"the comfort of a close friendship\", \"the bittersweet feeling of leaving home\",\n","    \"the excitement of traveling to a new country\", \"a moment of quiet reflection\", \"the struggle against injustice\",\n","    \"the feeling of being an outsider\", \"the feeling of 'information overload'\", \"the challenge of finding truth online\",\n","    \"the joy of reconnecting with an old friend\", \"the melancholy of a passing season in a city park\", \"the energy of a startup company\",\n","    \"the quiet satisfaction of a finished project\", \"the collective grief of a public tragedy\", \"the hope of a new beginning\",\n","    \"the struggle for work-life balance\", \"the feeling of being watched by algorithms\", \"the comfort of a pet waiting at home\",\n","    \"the anxiety of a global pandemic\", \"the excitement of a new scientific discovery\", \"the loneliness of a crowded room\",\n","    \"the peace of a solo hike in nature\", \"the feeling of 'deja vu'\", \"the impact of social media on society\",\n","    \"the nature of a digital footprint\", \"a modern election's tension\", \"the quiet of a house after a party\",\n","\n","    # Places & Scenes\n","    \"a bustling modern city at night\", \"a wind turbine farm\", \"solar panels on a roof\",\n","    \"a modern suspension bridge\", \"a music festival\", \"a quiet suburban neighborhood\",\n","    \"a bustling stock market floor\", \"a scientist in a cleanroom\", \"a modern art gallery\",\n","    \"a sports stadium during a game\", \"a power grid\", \"a hydroelectric dam\",\n","    \"a modern hospital emergency room\", \"a video blogger's studio\", \"an international space station\",\n","    \"a modern university lecture hall\", \"a recycling plant\", \"an automated warehouse\",\n","    \"a city's subway system\", \"a modern concert hall\", \"a modern farmer's market\",\n","    \"a skyscraper's observation deck\", \"a modern political debate\", \"a modern art museum\",\n","    \"a bustling international food market\", \"a quiet co-working space\", \"a massive container ship in a port\",\n","    \"a wind-swept coastal highway\", \"a university research library\", \"a high-tech greenhouse\",\n","    \"a neon-lit street in Tokyo\", \"a silent data archive\", \"a modern architectural marvel\",\n","    \"a solar-powered desert community\", \"a high-security government building\", \"a film festival red carpet\",\n","    \"a street art mural on a brick wall\", \"a large-scale music recording studio\", \"a quiet electric bicycle on a path\",\n","    \"a skyscraper's rooftop garden\", \"a digital map showing traffic\", \"a modern concert hall's acoustics\",\n","    \"a bustling public square\",\n","\n","    # Abstract Concepts\n","    \"a complex algorithm\", \"a global supply chain\", \"a digital news headline\", \"a streaming music playlist\",\n","    \"an online forum discussion\", \"the concept of a digital identity\", \"the nature of a computer virus\",\n","    \"the idea of a 'global village'\", \"the ethics of artificial intelligence\", \"the spread of a viral meme\",\n","    \"the complexity of the global financial system\", \"the privacy of personal data\",\n","    \"the theory of relativity explained simply\", \"the nature of open-source software\",\n","    \"the process of machine learning\", \"the idea of a multiverse\"\n","]\n","\n","prompt_template = \"Topic: {topic}\""],"metadata":{"id":"fRDkPeTXPvJY"},"id":"fRDkPeTXPvJY","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generating Poems\n","\n","try:\n","    with open(API_KEY_FILE, 'r') as f:\n","        api_key = f.read().strip()\n","    client = OpenAI(api_key=api_key)\n","\n","if client:\n","    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n","        print(f\"\\nStarting generation of {len(modern_topics)} poems...\")\n","\n","        for i, topic in enumerate(modern_topics[4:]):\n","            print(f\"--- Generating poem {i+1}/{len(modern_topics)}: '{topic}' ---\")\n","\n","            # Fill in the topic for the current iteration\n","            prompt = prompt_template.format(topic=topic)\n","\n","            try:\n","                # Make the API call to GPT-4o\n","                response = client.chat.completions.create(\n","                    model=\"gpt-4o\",\n","                    messages=[\n","                        {\"role\": \"system\", \"content\": \"You are a master of classical Latin poetry who writes in strict dactylic hexameter. Your task is to compose a 10-line Latin poem in dactylic hexameter about the topic provided by the user. You must only output the poem, with no introduction, translation, or commentary before and after.\"},\n","                        {\"role\": \"user\", \"content\": prompt}\n","                    ],\n","                    temperature=0.7,\n","                    max_tokens=200,\n","                )\n","\n","                # Extract the poem from the response\n","                poem = response.choices[0].message.content.strip()\n","\n","                # Append the new poem to the file, followed by a double newline\n","                f.write(poem + \"\\n\\n\")\n","                f.flush()\n","\n","            # Wait for a second to be respectful of the API rate limits\n","            time.sleep(1)\n","\n","    print(\"Done\")"],"metadata":{"id":"_7jHfkudPvHV"},"id":"_7jHfkudPvHV","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python (latin)","language":"python","name":"latin"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"colab":{"provenance":[{"file_id":"1cFQVdTgeo9idX9oGK2ozUVhsJtUUfTQs","timestamp":1755208820648}],"collapsed_sections":["Colt63hVfdbm","QyNqnyEZmpSq","WkQf5WqUopub","oJcG6HUaqHgd","rGYDBRRrJBUX","a3OJ3B4bMYIj","4BpyQBLpPn3d"]}},"nbformat":4,"nbformat_minor":5}