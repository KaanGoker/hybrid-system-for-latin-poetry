{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides a representative example of the fine-tuning process. The fundamental script and methodology were applied consistently across all other experiments, with adjustments made to the hyperparameters for each run.\n",
        "\n",
        "*   Model Quantization\n",
        "*   LoRA Configuration\n",
        "*   Training\n",
        "*   Experiment Tracking\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8xHqwkN6DYTR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning Gemma 3 12B IT Model"
      ],
      "metadata": {
        "id": "eyb3aHpkBNnh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w7dRSZiADHV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import wandb\n",
        "import shutil\n",
        "from datasets import load_from_disk\n",
        "from google.colab import drive\n",
        "from huggingface_hub import login\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    EarlyStoppingCallback,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive and Paths\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "model_path = \"/content/drive/MyDrive/ClassicalLatinPoetryGeneration/FinalModels/gemma-3-12b-it\"\n",
        "dataset_drive_path = r\"/content/drive/MyDrive/ClassicalLatinPoetryGeneration/DatasetV4-IT/DatasetV4/tokenized\"\n",
        "output_dir = r\"/content/drive/MyDrive/ClassicalLatinPoetryGeneration/models/gemma-3-12b-it-FT14\"\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "vQKVcklvBRiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "on0YSBEVBXsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantizing the Model, Loading Tokenizer\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "heLN0AeuBXqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuring Lora Adapters\n",
        "lora_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=128,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Checking out trainable parameters\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "z9_p4cx8BZ3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset\n",
        "\n",
        "local_dataset_path = \"/content/dataset_local_12b_v4\"\n",
        "if os.path.exists(local_dataset_path):\n",
        "    shutil.rmtree(local_dataset_path)\n",
        "shutil.copytree(dataset_drive_path, local_dataset_path)\n",
        "\n",
        "tokenized_dataset = load_from_disk(local_dataset_path)"
      ],
      "metadata": {
        "id": "X2QWv5_9BbgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining W&B run configuration\n",
        "run_config = {\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"lr_scheduler_type\": \"cosine\",\n",
        "    \"epochs\": 20,\n",
        "    \"r\": lora_config.r,\n",
        "    \"lora_alpha\": lora_config.lora_alpha,\n",
        "    \"early_stopping_patience\": 3,\n",
        "    \"dataset_version\": \"V4-gemma-12b-it\",\n",
        "    \"batch_size\": 4,\n",
        "    \"gradient_accumulation\": 4,\n",
        "    \"double_quant\": False\n",
        "}\n",
        "\n",
        "# New name for the Run:\n",
        "run_name = f\"gemma-3-12b-it_FT14_DSv4_LR{run_config['learning_rate']}_R{run_config['r']}_B{run_config['batch_size']}GA{run_config['gradient_accumulation']}_MOREMODULE_MOREBATCH\"\n",
        "wandb.init(\n",
        "    project=\"latin-hexameter-thesis\",\n",
        "    name=run_name,\n",
        "    config=run_config\n",
        ")"
      ],
      "metadata": {
        "id": "fgskAHjlBgVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Trainer class to log perplexity\n",
        "class MyTrainer(Trainer):\n",
        "    def evaluate(self, *args, **kwargs):\n",
        "        metrics = super().evaluate(*args, **kwargs)\n",
        "        try:\n",
        "            perplexity = math.exp(metrics[\"eval_loss\"])\n",
        "            custom_metrics = {\"eval_perplexity\": perplexity}\n",
        "            self.log(custom_metrics)\n",
        "        return metrics\n",
        "\n",
        "# Training arguments for QLoRA\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=run_config[\"epochs\"],\n",
        "    learning_rate=run_config[\"learning_rate\"],\n",
        "    lr_scheduler_type=run_config[\"lr_scheduler_type\"],\n",
        "    per_device_train_batch_size=run_config[\"batch_size\"],\n",
        "    gradient_accumulation_steps=run_config[\"gradient_accumulation\"],\n",
        "    bf16=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    save_total_limit=4,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"wandb\",\n",
        ")\n",
        "\n",
        "# Initializing\n",
        "trainer = MyTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=run_config[\"early_stopping_patience\"])]\n",
        ")"
      ],
      "metadata": {
        "id": "4A6vnIhSBgSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "Xo-gy0RVBgQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "-ipplaj6BjMw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}